{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b340a41b-84b9-404d-b6b2-9c0c036f7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "D_VAE_loss=[]\n",
    "LR_loss=[]\n",
    "G_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09d956-6203-491e-a6f8-ce89fd5c3bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoch=0, n_epochs=90, dataset_name='sample', batch_size=16, lr=0.0002, b1=0.5, b2=0.999, n_cpu=3, img_height=256, img_width=256, channels=3, latent_dim=8, sample_interval=500, checkpoint_interval=40, lambda_pixel=10, lambda_latent=0.5, lambda_kl=0.01)\n",
      "CUDAが使えます\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toyou\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/90] [Batch 1122/1125] [D VAE_loss: 1.390362, LR_loss: 1.154687] [G loss: 3.281095, pixel: 0.138301, kl: 0.001682, latent: 0.357105] ETA: 17:54:44.9996946.071144"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epoch\", type=int, default=0, help=\"トレーニングを開始するためのエポック\")\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=90, help=\"トレーニングエポック数\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"sample\", help=\"データセットの名前\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"バッチサイズ\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: 学習速度\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: 勾配の減衰\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=3, help=\"CPUスレッド数\")\n",
    "parser.add_argument(\"--img_height\", type=int, default=256, help=\"画像サイズの高さ\")\n",
    "parser.add_argument(\"--img_width\", type=int, default=256, help=\"画像サイズの横\")\n",
    "parser.add_argument(\"--channels\", type=int, default=3, help=\"画像チャネル数\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=8, help=\"潜伏次元数（ノイズベクトル？）\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=500, help=\"interval between saving generator samples\")\n",
    "parser.add_argument(\"--checkpoint_interval\", type=int, default=40, help=\"interval between model checkpoints\")\n",
    "parser.add_argument(\"--lambda_pixel\", type=float, default=10, help=\"pixelwise loss weight\")\n",
    "parser.add_argument(\"--lambda_latent\", type=float, default=0.5, help=\"latent loss weight\")\n",
    "parser.add_argument(\"--lambda_kl\", type=float, default=0.01, help=\"kullback-leibler loss weight\")\n",
    "opt = parser.parse_args(args=[])\n",
    "print(opt)\n",
    "\n",
    "os.makedirs(\"images/%s\" % opt.dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % opt.dataset_name, exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "input_shape = (opt.channels, opt.img_height, opt.img_width)\n",
    "\n",
    "\n",
    "\n",
    "# Loss関数\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator, encoder and discriminators\n",
    "generator = Generator(opt.latent_dim, input_shape)\n",
    "encoder = Encoder(opt.latent_dim, input_shape)\n",
    "D_VAE = MultiDiscriminator(input_shape)\n",
    "D_LR = MultiDiscriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    print(\"CUDAが使えます\")\n",
    "    generator = generator.cuda()\n",
    "    encoder.cuda()\n",
    "    D_VAE = D_VAE.cuda()\n",
    "    D_LR = D_LR.cuda()\n",
    "    mae_loss.cuda()\n",
    "\n",
    "if opt.epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    encoder.load_state_dict(torch.load(\"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_VAE.load_state_dict(torch.load(\"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "    D_LR.load_state_dict(torch.load(\"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, opt.epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    D_VAE.apply(weights_init_normal)\n",
    "    D_LR.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "################################################################\n",
    "\n",
    "#画像の読み込み\n",
    "\n",
    "###################################################################\n",
    "\n",
    "#画像の前処理方法の設定\n",
    "transforms_ = [\n",
    "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "]\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "# #trainに使う画像と、valに使う画像を分ける。\n",
    "# train_A, train_B = make_datapath_list(opt)\n",
    "# val_A, val_B = make_datapath_list(opt, phase=\"val\")\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     #ImageDatasetはDatasetクラスをオリジナルに改変したもの->datasets.py\n",
    "#     ImageDataset(train_A, train_B, opt, transforms_=transforms_),\n",
    "#     batch_size=opt.batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=opt.n_cpu,\n",
    "# )\n",
    "\n",
    "# val_dataloader = DataLoader(\n",
    "#     ImageDataset(val_A, val_B, opt, transforms_=transforms_),\n",
    "#     batch_size=opt.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(\"./data/%s\" % opt.dataset_name, input_shape, mode=\"train\" ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"./data/%s\" % opt.dataset_name, input_shape, mode=\"val\"),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "\n",
    "#####################################################\n",
    "\n",
    "#画像保存関数\n",
    "\n",
    "###################################################\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    generator.eval()\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    img_samples = None\n",
    "    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n",
    "        # Repeat input image by number of desired columns\n",
    "        real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n",
    "        real_A = Variable(real_A.type(Tensor))\n",
    "        # Sample latent representations\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n",
    "        # Generate samples\n",
    "        fake_B = generator(real_A, sampled_z)\n",
    "        # Concatenate samples horisontally\n",
    "        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "        img_sample = torch.cat((img_A, fake_B), -1)\n",
    "        img_sample = img_sample.view(1, *img_sample.shape)\n",
    "        # Concatenate with previous samples vertically\n",
    "        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
    "    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n",
    "    generator.train()\n",
    "\n",
    "\n",
    "    #再パラメータ化\n",
    "def reparameterization(mu, logvar):\n",
    "    std = torch.exp(logvar / 2)\n",
    "    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n",
    "    z = sampled_z * std + mu\n",
    "    return z\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "#  学習\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# Adversarial loss\n",
    "valid = 1\n",
    "fake = 0\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(opt.epoch, opt.n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
    "        # print(real_A.size())\n",
    "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Train Generator and Encoder\n",
    "        # -------------------------------\n",
    "\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # ----------\n",
    "        # cVAE-GAN\n",
    "        # ----------\n",
    "\n",
    "        # Produce output using encoding of B (cVAE-GAN)\n",
    "        # print(real_B.size())\n",
    "        mu, logvar = encoder(real_B)     #######################ここでエラーが\n",
    "        # print(mu)\n",
    "        # print(logvar)\n",
    "        encoded_z = reparameterization(mu, logvar)\n",
    "        # print(encoded_z)\n",
    "        fake_B = generator(real_A, encoded_z)\n",
    "\n",
    "        # Pixelwise loss of translated image by VAE\n",
    "        loss_pixel = mae_loss(fake_B, real_B)\n",
    "        # Kullback-Leibler divergence of encoded B\n",
    "        loss_kl = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - logvar - 1)\n",
    "        # Adversarial loss\n",
    "        loss_VAE_GAN = D_VAE.compute_loss(fake_B, valid)\n",
    "\n",
    "        # ---------\n",
    "        # cLR-GAN\n",
    "        # ---------\n",
    "\n",
    "        # Produce output using sampled z (cLR-GAN)\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (real_A.size(0), opt.latent_dim))))\n",
    "        # print(sampled_z.size())\n",
    "        _fake_B = generator(real_A, sampled_z)\n",
    "        # print(_fake_B.size())\n",
    "        # cLR Loss: Adversarial loss\n",
    "        loss_LR_GAN = D_LR.compute_loss(_fake_B, valid)\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Total Loss (Generator + Encoder)\n",
    "        # ----------------------------------\n",
    "\n",
    "        loss_GE = loss_VAE_GAN + loss_LR_GAN + opt.lambda_pixel * loss_pixel + opt.lambda_kl * loss_kl\n",
    "\n",
    "        loss_GE.backward(retain_graph=True)\n",
    "        optimizer_E.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Generator Only Loss\n",
    "        # ---------------------\n",
    "\n",
    "        # Latent L1 loss\n",
    "        _mu, _ = encoder(_fake_B)\n",
    "        loss_latent = opt.lambda_latent * mae_loss(_mu, sampled_z)\n",
    "\n",
    "        loss_latent.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ----------------------------------\n",
    "        #  Train Discriminator (cVAE-GAN)\n",
    "        # ----------------------------------\n",
    "\n",
    "        optimizer_D_VAE.zero_grad()\n",
    "\n",
    "        loss_D_VAE = D_VAE.compute_loss(real_B, valid) + D_VAE.compute_loss(fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_VAE.backward()\n",
    "        optimizer_D_VAE.step()\n",
    "\n",
    "        # ---------------------------------\n",
    "        #  Train Discriminator (cLR-GAN)\n",
    "        # ---------------------------------\n",
    "\n",
    "        optimizer_D_LR.zero_grad()\n",
    "\n",
    "        loss_D_LR = D_LR.compute_loss(real_B, valid) + D_LR.compute_loss(_fake_B.detach(), fake)\n",
    "\n",
    "        loss_D_LR.backward()\n",
    "        optimizer_D_LR.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = opt.n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D VAE_loss: %f, LR_loss: %f] [G loss: %f, pixel: %f, kl: %f, latent: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                opt.n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D_VAE.item(),\n",
    "                loss_D_LR.item(),\n",
    "                loss_GE.item(),\n",
    "                loss_pixel.item(),\n",
    "                loss_kl.item(),\n",
    "                loss_latent.item(),\n",
    "                time_left,\n",
    "                \n",
    "            )\n",
    "        )\n",
    "        D_VAE_loss.append(loss_D_VAE.item())\n",
    "        LR_loss.append(loss_D_LR.item())\n",
    "        G_loss.append(loss_GE.item())\n",
    "\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "        if epoch % opt.checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "            torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (opt.dataset_name, epoch))\n",
    "            torch.save(encoder.state_dict(), \"saved_models/%s/encoder_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        # torch.save(D_VAE.state_dict(), \"saved_models/%s/D_VAE_%d.pth\" % (opt.dataset_name, epoch))\n",
    "        # torch.save(D_LR.state_dict(), \"saved_models/%s/D_LR_%d.pth\" % (opt.dataset_name, epoch))\n",
    "\n",
    "        \n",
    "torch.save(generator.state_dict(), \"saved_models/%s/generator_last.pth\" % (opt.dataset_name))\n",
    "torch.save(encoder.state_dict(), \"saved_models/%s/encoder_last.pth\" % (opt.dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d3d15-deb5-4141-8031-a019eb5cc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "D_VAE_loss_DF=pd.DataFrame(D_VAE_loss,columns={\"D_VAE_loss\"})\n",
    "LR_loss_DF=pd.DataFrame(LR_loss,columns={\"LR_loss\"})\n",
    "G_loss_DF=pd.DataFrame(G_loss,columns={\"G_loss\"})\n",
    "\n",
    "\n",
    "loss_all=pd.DataFrame()\n",
    "loss_all=pd.concat([loss_all,D_VAE_loss_DF],axis=0)\n",
    "loss_all=pd.concat([loss_all,LR_loss_DF],axis=1)\n",
    "loss_all=pd.concat([loss_all,G_loss_DF],axis=1)\n",
    "\n",
    "\n",
    "loss_all.to_csv(\"./bicycle_Loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9adb2-8a02-4430-86ca-3a58a521716a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
